{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Current To-Do:\n",
    "- Contrast map comparison\n",
    "- Conjunction analysis\n",
    "\n",
    "Backburner questions:\n",
    "- XCP-D?\n",
    "- DiFuMo atlas instead of Schaefer?\n",
    "- Different SRM distance penalties (distance as penalty instead of parcelwise? Searchlights instead of parcels?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[ROADMAP DOC](https://docs.google.com/document/d/13P4QTHxrT5lZfCOXtN59xCKpJfnObtqh3uZkuRqPxR4/edit?pli=1#heading=h.2qncjqtc0b5j)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load all data (connectomes, task) and subjects\n",
    "# loop through subjects\n",
    "#   derive loso SRM transforms\n",
    "#   transform task data\n",
    "#   compute conjunction\n",
    "# plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, glob, json, itertools\n",
    "import numpy as np\n",
    "from scipy.stats import pearsonr\n",
    "import matplotlib.pyplot as plt\n",
    "from nilearn.maskers import NiftiMasker\n",
    "from utils import srm_brainiak as srm\n",
    "from joblib import Parallel, delayed\n",
    "# sys.path.append(os.path.abspath(os.path.join(os.path.dirname(__file__), '..')))\n",
    "from connectivity import get_combined_mask\n",
    "from srm import load_parcel_map\n",
    "from utils.fastsrm_brainiak import FastSRM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTRAST_PATH = '/oak/stanford/groups/russpold/data/network_grant/discovery_BIDS_21.0.1/derivatives/output_optcom_MNI/'\n",
    "target_contrasts = {\n",
    "    'cuedTS': 'cuedTS_contrast-task_switch_cost',\n",
    "    'directedForgetting': 'directedForgetting_contrast-neg-con',\n",
    "    'flanker': 'flanker_contrast-incongruent - congruent',\n",
    "    'nBack': 'nBack_contrast-twoBack-oneBack',\n",
    "    'spatialTS': 'spatialTS_contrast-task_switch_cost',\n",
    "    'shapeMatching': 'shapeMatching_contrast-main_vars',\n",
    "    'goNogo': 'goNogo_contrast-nogo_success-go',\n",
    "    'stopSignal': 'stopSignal_contrast-stop_failure-go'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONNECTOME_PATH = '/scratch/users/csiyer/connectomes/'\n",
    "SRM_PATH = '/scratch/users/csiyer/srm_outputs/'\n",
    "\n",
    "connectome_files = sorted(glob.glob('/scratch/users/csiyer/connectomes/*avg*'))\n",
    "connectomes = [np.load(f) for f in connectome_files]\n",
    "sub_list = [f[f.find('sub')+4:f.find('sub')+7] for f in connectome_files] # s03, s10, etc.\n",
    "parcel_map = load_parcel_map(n_dimensions = 100)\n",
    "\n",
    "def compute_loso_srm(data_list, sub_list, loso_sub, parcel_map, n_features=100, save=True):\n",
    "    \"\"\"\n",
    "    This function uses BrainIAK's Shared Response Modeling function to compute parcel-wise SRMs (one per parcel, as an anatomical constraint).\n",
    "    CRUCIALLY, the shared model is derived on all subjects but one; that left-out subject is subsequently added to derive their own transformation matrix.\n",
    "    This is done for the purpose of eliminating data leakage when we later transform all other subjects' data into that subject's native space.\n",
    "\n",
    "    I use joblib's Parallel and delayed functions to speed up the process.\n",
    "    \n",
    "    Inputs: \n",
    "        - list of subject-specific connectomes\n",
    "        - list of subject names\n",
    "        - which subject to leave out\n",
    "        - parcel map matching the data dimensionality\n",
    "        - how many dimensions in the shared model\n",
    "        - whether to save the transformation matrices\n",
    "    Output: transformation matrices for each subject\n",
    "\n",
    "    NOTE: the parcelwise shared responses are not saved here, because I haven't been using them for anything.\n",
    "    \"\"\"\n",
    "    outpath = os.path.join(SRM_PATH, f'loso/{loso_sub}/')\n",
    "    if os.path.exists(outpath): # if this script has been run before, we can load past results instead of re-deriving\n",
    "        return [np.load(glob.glob(outpath + f'sub-{sub}_srm_transform_loso-{loso_sub}.npy')[0]) for sub in sub_list]\n",
    "\n",
    "    loso_idx = sub_list.index(loso_sub)\n",
    "    loso_data = data_list[loso_idx]\n",
    "    train_subs = [s for i,s in enumerate(sub_list) if i != loso_idx]\n",
    "    train_data = [d for i,d in enumerate(data_list) if i != loso_idx]\n",
    "    new_subject_list = [s for s in sub_list if s != loso_sub] + [loso_sub]\n",
    "\n",
    "    def single_parcel_srm(train_data, loso_data, parcel_map, parcel_label, n_features):\n",
    "        parcel_idxs = np.where(parcel_map == parcel_label)\n",
    "        train_data_parcel = [d[parcel_idxs] for d in train_data]\n",
    "        srm = FastSRM(n_components=n_features, n_iter=20, n_jobs=1, aggregate='mean')\n",
    "        reduced_sr = srm.fit_transform(train_data)\n",
    "        srm.aggregate = None\n",
    "        srm.add_subjects(loso_data, reduced_sr)\n",
    "\n",
    "\n",
    "\n",
    "    def single_parcel_srm(data_list, parcel_map, parcel_label, n_features):\n",
    "        parcel_idx = np.where(parcel_map == parcel_label)\n",
    "        data_parcel = [d[parcel_idx] for d in data_list]\n",
    "        shared_model = srm.SRM(n_iter=20, features=n_features) \n",
    "        shared_model.fit(data_parcel)\n",
    "        return shared_model.s_, shared_model.w_, parcel_idx\n",
    "\n",
    "    srm_outputs = Parallel(n_jobs=-1)(\n",
    "        delayed(single_parcel_srm)(data_list, parcel_map, parcel_label, n_features) for parcel_label in np.sort(np.unique(parcel_map))\n",
    "    )\n",
    "\n",
    "    parcelwise_shared_responses = [s[0] for s in srm_outputs] # concatenate all the parcelwise shared space responses/connectivities\n",
    "\n",
    "    subject_transforms = [np.zeros((data_list[0].shape[0], n_features)) for i in range(len(data_list))] # empty initalize\n",
    "\n",
    "    for _, w_, parcel_idx in srm_outputs: # concatenate transforms into subject-wise all-voxel transformation matrices \n",
    "        for i,sub in enumerate(subject_transforms):\n",
    "            sub[parcel_idx,:] = w_[i]\n",
    "    # i know there's a better way to do that ^ with more linear algebra. urgh\n",
    "\n",
    "    if save:\n",
    "        np.save('/scratch/users/csiyer/parcelwise_shared_responses.npy', parcelwise_shared_responses)\n",
    "        for i,sub in enumerate(subject_transforms):\n",
    "            np.save(f'/scratch/users/csiyer/{sub_list[i]}_srm_transform.npy', sub)\n",
    "    \n",
    "    return subject_transforms, parcelwise_shared_responses \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
