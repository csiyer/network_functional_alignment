{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook basically contains a summary of what the other scripts here do, in one place to bring out the exact next step that I'm trying to accomplish.\n",
    "\n",
    "- First, I load a voxel- and parcel-wise timeseries from our data.\n",
    "- Then, I create functional conncetomes from those (correlating each voxel to each parcel // parcels as the connectivity targets).\n",
    "- I didn't copy the details here but we'll assess the reliability of these connectomes in a separate script.\n",
    "- Now, I want to write code to derive parcel-wise SRMs from a subject-averaged connectome and concatenate transformation matrices to create a brain-wide transformation matrix for each subject. \n",
    "\n",
    "My main question is that once I have read in voxel data through a masker, I no longer know how to keep track of which voxels correspond to which labels in the Schaefer maps we are using here. Somewhat relatedly, I'm not sure exactly what to mask with, and if I should be using the Schaefer parcellation to do so.\n",
    "\n",
    "The input files are resting state functional data, already fMRIPrepped and in mni152nlin2009casym_res-2 space. \n",
    "\n",
    "\n",
    "Other to-do (just notes for myself):\n",
    "- potentially write searchlight loading to try searchlights as connectivity targets\n",
    "- potentially use FCMA library to have voxels as our connectivity targets (Cython issues...)\n",
    "- Reliability.py: permutation tests?\n",
    "- Write task_decoding.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import numpy as np\n",
    "from nilearn import datasets\n",
    "from nilearn.maskers import MultiNiftiMasker, MultiNiftiLabelsMasker, NiftiSpheresMasker\n",
    "from scipy.stats import pearsonr\n",
    "from math import tanh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(FILE_PATHS=[], \n",
    "            strategy = 'parcel', \n",
    "            schaefer_n_rois=400, \n",
    "            sphere_radius=8, \n",
    "            sphere_spacing=6):\n",
    "    \"\"\"\n",
    "    This function loads functional data in 3 different ways:\n",
    "    1. Strategy = voxel: extracts direct voxel timeseries\n",
    "    2. Strategy = parcel: extracts parcel timeseries from schaefer 2018 atlas with a given # of ROIs (400 for now)\n",
    "    3. (NOT IMPLEMENTED) Strategy = searchlight: extracts timeseries of spheres of a given radius and spacing\n",
    "\n",
    "    NOTE to self: the masker.fit_transform functions return an array of the shape (n_TRs x n_voxels)\n",
    "    \"\"\"\n",
    "\n",
    "    if FILE_PATHS == []:\n",
    "        FILE_PATHS = glob.glob('data/rest/*.nii.gz') # all rest data in my current testing dir by default\n",
    "\n",
    "    masker_args = {\n",
    "        'standardize': 'zscore_sample', # ?\n",
    "        'n_jobs': -1,\n",
    "        # add: mask_img from fmriprep brain mask?\n",
    "        # not doing any: smoothing, detrend, standardize, low_pass, high_pass, t_r\n",
    "    }\n",
    "\n",
    "    if strategy == 'voxel':\n",
    "        masker = MultiNiftiMasker(**masker_args) # mask_strategy = 'whole-brain-template' or 'gm-template'?\n",
    "\n",
    "    elif strategy == 'parcel':\n",
    "        schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=schaefer_n_rois, \n",
    "                                                        yeo_networks=7, \n",
    "                                                        resolution_mm=2, # because our data is too\n",
    "                                                        data_dir='data/schaefer', \n",
    "                                                        verbose=0)\n",
    "        masker = MultiNiftiLabelsMasker(labels_img = schaefer_atlas.maps,\n",
    "                                labels = schaefer_atlas.labels,\n",
    "                                resampling_target = 'data',\n",
    "                                strategy = 'mean',\n",
    "                                **masker_args)\n",
    "\n",
    "    elif strategy == 'searchlight':\n",
    "        sphere_coords = [] # get the centerpoint coordinates of spheres - these current numbers are incorrect\n",
    "        # movie magic\n",
    "        masker = NiftiSpheresMasker(seeds = sphere_coords, \n",
    "                                    radius=sphere_radius, \n",
    "                                    **masker_args)\n",
    "        return [masker.fit_transform(f) for f in FILE_PATHS]\n",
    "\n",
    "    # this only works for the multimaskers with voxel/parcel\n",
    "    return masker.fit_transform(FILE_PATHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def correlate_rows(mat1, mat2, zscore=False):\n",
    "    \"\"\" \n",
    "    Helper function for below\n",
    "    Returns a matrix with the Pearson r correlation of each column (voxel) of mat1 with each column (parcel/target) of mat2\n",
    "    \"\"\"\n",
    "    correlation_matrix = np.empty((mat1.shape[1], mat2.shape[1]))\n",
    "    for i in range(mat1.shape[0]):\n",
    "        for j in range(mat2.shape[0]):\n",
    "            correlation_matrix[i, j] = pearsonr(mat1[:, i], mat2[:, j])[0]\n",
    "            if zscore:\n",
    "                # fisher transformation\n",
    "                correlation_matrix[i,j] = tanh(correlation_matrix[i,j])\n",
    "    return correlation_matrix\n",
    "\n",
    "def compute_fc_target(voxel_timeseries, target_timeseries, zscore=True):\n",
    "    \"\"\" \n",
    "    This will take each column in the voxel timeseries (across all the TRs/rows) \n",
    "    and correlate it with each column in the target timeseries.\n",
    "    Connectivity targets could be the parcel timeseries, or a searchlight timeseries.\n",
    "\n",
    "    Not using nilearn ConnectivityMeasure because this is across two matrices--couldn't figure that out\n",
    "    \n",
    "    NOTE: The connectivity target in which a given voxel resides is not excluded yet -- should it be?\n",
    "    \"\"\"\n",
    "    return [correlate_rows(voxel_timeseries[i], target_timeseries[i], zscore) for i in range(len(voxel_timeseries))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data, calculate connectome\n",
    "voxel_data = load_data(strategy='voxel') \n",
    "parcel_data = load_data(strategy='parcel')\n",
    "connectomes = compute_fc_target(voxel_data, parcel_data, zscore = True)\n",
    "# there will be a step in here where the connectomes from each subject are averaged across sessions--ignore that for now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "now here is my problem\n",
    "\n",
    "i have connectomes of the shape (n_voxels x n_parcels) to derive SRMs from \n",
    "but, I have lost the ability to know which parcel each voxel belongs to which parcel.\n",
    "\n",
    "The ideas that occur to me:\n",
    "- If I better understood exactly what mask is applied to the data by the masker and how it's flattened, then I can resample the atlas to the original data shape/affine and then mask and flatten it in the same way.\n",
    "- Should I be using the nilearn maskers at all? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "mock-up of what I want to run:\n",
    "\n",
    "schaefer_atlas = datasets.fetch_atlas_schaefer_2018(n_rois=400, \n",
    "                                                        yeo_networks=7, \n",
    "                                                        resolution_mm=2, # because our data is too\n",
    "                                                        data_dir='data', \n",
    "                                                        verbose=0)\n",
    "                                                        \n",
    "label_map = nib.load(schaefer_atlas.maps)\n",
    "shared_model = srm.SRM(n_iter=20, features=50)\n",
    "transforms_list = []\n",
    "\n",
    "for parcel in np.unique(label_map):\n",
    "    parcel_idx = np.where(label_map==parcel)\n",
    "    parcel_connectomes = [subj_connectome[parcel_idx] for subj_connectome in connectomes]   \n",
    "\n",
    "    shared_model.fit(parcel_connectomes)   \n",
    "    transforms_list.append(shared_model.w_) # save the transformation matrices to concatenate later\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
