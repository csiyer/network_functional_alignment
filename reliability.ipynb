{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook assesses the within-subject reliability of functional connectivity matrices \n",
    "created in connectivity.py (one for each session/subject, 12 sessions/subject).\n",
    "\n",
    "Similarity measure: Pearson spatial correlation (?)\n",
    "From Finn et al. 2015 Connectome Fingerprinting \n",
    "https://www.nature.com/articles/nn.4135\n",
    "\n",
    "\n",
    "3 strategies:\n",
    "- Session-by-session (compare similarity of all pairs of connectomes, do we see off-diagonal subject-specific structure?)\n",
    "- Intra-class correlation coefficient\n",
    "- Split-half (split each subject's data in half, average, and then compare. Somehow.)\n",
    "\n",
    "Author: Chris Iyer, updated 8/11/2023"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load libraries and connectomes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import seaborn as sns\n",
    "sns.set(palette=\"colorblind\")\n",
    "import matplotlib.pyplot as plt\n",
    "from pingouin import intraclass_corr\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load connectomes\n",
    "def load_connectomes():\n",
    "\n",
    "    data_dict = {}\n",
    "    for sub in np.unique([f[f.find('sub'):f.find('sub')+7] for f in glob.glob('output/connectomes/*ses*')]):\n",
    "        data_dict[sub] = {}\n",
    "        for ses in np.unique([f[f.find('ses'):f.find('ses')+7] for f in glob.glob(f'output/connectomes/*{sub}*ses*')]):\n",
    "            curr = np.load(glob.glob(f'output/connectomes/*{sub}_{ses}*')[0])\n",
    "            data_dict[sub][ses] = {\n",
    "                'connectome': curr,\n",
    "                'connectome_flat': curr[np.triu_indices(curr)]\n",
    "            }\n",
    "\n",
    "    # also create unlabeled lists\n",
    "    connectomes_flat = []\n",
    "    for sub in data_dict.keys():\n",
    "        for ses, data in data_dict[sub].items():\n",
    "            connectomes_flat.append(data['connectome_flat'])\n",
    "    \n",
    "    return data_dict, connectomes_flat\n",
    "\n",
    "def load_dummy_data():\n",
    "    # load connectomes\n",
    "    data_dict = {}\n",
    "    for sub in ['sub-s01', 'sub-s02', 'sub-s03', 'sub-s04', 'sub-s05']:\n",
    "        data_dict[sub] = {}\n",
    "        for ses in ['ses-01', 'ses-02', 'ses-03', 'ses-04']:\n",
    "            curr = np.random.rand(10, 5)\n",
    "            data_dict[sub][ses] = {\n",
    "                'connectome': curr,\n",
    "                'connectome_flat': curr[np.triu_indices(n=curr.shape[0], m=curr.shape[1])]\n",
    "            }\n",
    "\n",
    "    # also create unlabeled lists\n",
    "    connectomes_flat = []\n",
    "    for sub in data_dict.keys():\n",
    "        for ses, data in data_dict[sub].items():\n",
    "            connectomes_flat.append(data['connectome_flat'])\n",
    "\n",
    "    return data_dict, connectomes_flat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data_dict, connectomes_flat = load_connectomes()\n",
    "data_dict, connectomes_flat = load_dummy_data() \n",
    "n_subjects = len(data_dict.keys())\n",
    "n_sessions = len(data_dict['sub-s01'].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Compare similarity session-by-session of all pairs of connectomes\n",
    "Do we see off-diagonal subject-specific structure?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate correlation matrix of connectivity vectors\n",
    "similarity_matrix = np.corrcoef(connectomes_flat)\n",
    "\n",
    "# plot the similarity matrix \n",
    "fig, ax = plt.subplots(1, 1, figsize=(10,8))\n",
    "\n",
    "sns.heatmap(similarity_matrix, ax=ax)\n",
    "\n",
    "# Add lines to box off every num_ses entries (each subject)\n",
    "for i in range(0, similarity_matrix.shape[0], n_sessions):\n",
    "    ax.axhline(i, color='white', linewidth=2)\n",
    "    ax.axvline(i, color='white', linewidth=2)\n",
    "\n",
    "xtick_labels = list(data_dict.keys())\n",
    "xtick_positions = np.arange(n_sessions/2, similarity_matrix.shape[1], n_sessions)\n",
    "ax.set_xticks(xtick_positions)\n",
    "ax.set_xticklabels(xtick_labels)\n",
    "ax.set_yticks(xtick_positions)\n",
    "ax.set_yticklabels(xtick_labels)\n",
    "ax.set_title(\"Session-wise Sonnectivity Similarity Matrix\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Intra-class Correlation Coefficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.array(connectomes_flat)\n",
    "data_long = pd.DataFrame(data.flatten(), columns=['connectivity_values'])\n",
    "data_long['subjects'] = np.repeat(range(n_subjects), n_sessions*data.shape[1]) \n",
    "data_long['sessions'] = np.tile(np.repeat(range(n_sessions), data.shape[1]), n_subjects)  # 12 sessions for each subject\n",
    "\n",
    "icc = intraclass_corr(data=data_long, targets='subjects', raters='sessions', ratings='connectivity_values').set_index('Type')\n",
    "print(icc.loc['ICC2'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Split-half reliability\n",
    "Procedure:\n",
    "1. Split every subject's sessions randomly into 2 groups of 6 sessions\n",
    "2. Average within a group (now we have 2 connectomes per subject)\n",
    "3. Calculate the correlation of each to its within-subject pair\n",
    "4. Calculate the correlation of each to all the others, and average\n",
    "5. Iterate steps 1-4 1000 times and save all the values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 1000\n",
    "avg_within_sub_corr = []\n",
    "avg_across_sub_corr = []\n",
    "\n",
    "for i_iter in range(n_iter):\n",
    "\n",
    "    # split data into halves and calculate the mean of the half\n",
    "    split_dict = {} # for storing values during these calculations\n",
    "\n",
    "    for sub in data_dict.keys():\n",
    "        split_dict[sub] = {}\n",
    "        rand_ses = list(np.random.permutation(list(data_dict[sub].keys())))\n",
    "        group1data = []\n",
    "        group2data = []\n",
    "\n",
    "        for ses, data in data_dict[sub].items():\n",
    "            if rand_ses.index(ses) > len(rand_ses)/2-1:\n",
    "                # if in the second half of randomized list\n",
    "                group1data.append(data['connectome_flat'])\n",
    "            else:\n",
    "                group2data.append(data['connectome_flat'])\n",
    "        \n",
    "        split_dict[sub]['group1_mean'] = np.mean(group1data, axis=0)\n",
    "        split_dict[sub]['group2_mean'] = np.mean(group2data, axis=0)\n",
    "\n",
    "    # iterate again and calculate correlation with each other group\n",
    "    within_sub_corr = []\n",
    "    across_sub_corr = []\n",
    "    for sub in split_dict.keys():\n",
    "        # save correlation of that sub's two halves\n",
    "        within_sub_corr.append(np.corrcoef(split_dict[sub]['group1_mean'], split_dict[sub]['group2_mean']))\n",
    "\n",
    "        # calculate correlation of each of those to all others \n",
    "        for sub_two in split_dict.keys():\n",
    "            if sub_two != sub:\n",
    "                across_sub_corr.append(np.corrcoef(split_dict[sub]['group1_mean'], split_dict[sub_two]['group1_mean']))\n",
    "                across_sub_corr.append(np.corrcoef(split_dict[sub]['group1_mean'], split_dict[sub_two]['group2_mean']))\n",
    "                across_sub_corr.append(np.corrcoef(split_dict[sub]['group2_mean'], split_dict[sub_two]['group1_mean']))\n",
    "                across_sub_corr.append(np.corrcoef(split_dict[sub]['group2_mean'], split_dict[sub_two]['group2_mean']))\n",
    "\n",
    "    avg_within_sub_corr.append(np.mean(within_sub_corr))\n",
    "    avg_across_sub_corr.append(np.mean(across_sub_corr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the results\n",
    "fig, ax = plt.subplots(1,1)\n",
    "fig.suptitle('Within-subject vs. across-subject split-half connectome reliability across 1000 iterations')\n",
    "ax.boxplot(avg_within_sub_corr, positions=[1], patch_artist=True, boxprops=dict(facecolor='blue'), labels=['Within-subject'])\n",
    "ax.boxplot(avg_within_sub_corr, positions=[2], patch_artist=True, boxprops=dict(facecolor='red'), labels=['Across-subject'])\n",
    "ax.set_ylabel('Pearson r of split halves')\n",
    "ax.set_ylim(0,1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Finally, average connectomes within subject and re-save (if they are reliable!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for sub in data_dict.keys():\n",
    "    connectome_avg = np.mean([data_dict[sub][ses]['connectome'] for ses in data_dict[sub].keys()], axis=0)\n",
    "    np.save(f\"outputs/connectomes/{sub}_connectome_avg.npy\", connectome_avg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lab",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
